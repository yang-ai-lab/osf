<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="OSF: On Pre-training and Scaling of Sleep Foundation Models">
  <meta name="keywords" content="Foundation Model, Polysomnography, Self-Supervised Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OSF: On Pre-training and Scaling of Sleep Foundation Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zitao-shuai.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://yang-ai-lab.github.io/SleepLM_website/">
            Sleep Language Model
          </a>
          <a class="navbar-item" href="https://yang-ai-lab.github.io/SleepLM_website/">
            LLM Reasoning on Health Time-Series
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OSF: On Pre-training and Scaling of Sleep Foundation Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zitao-shuai.github.io/">Zitao Shuai</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/Zongzhe-Xu/">Zongzhe Xu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/daviddongjunyang">David Yang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://web.cs.ucla.edu/~weiwang/">Wei Wang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://web.cs.ucla.edu/~yuzhe/">Yuzhe Yang</a><sup>1</sup>,
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Los Angeles</span>
            <span class="author-block"><sup>2</sup>Emory University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/11duLamOhiX5FKfZxcfpYiMRg6TAxvGEK/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>


              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/yang-ai-lab/OSF-Open-Sleep-Foundation-Model"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>

            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body teaser-body">
      <figure class="image teaser-media">
        <img src="./static/images/1_radar.svg" alt="Radar" />
      </figure>
      <p class="subtitle has-text-centered">
        Figure 1: OSF consistently achieves state-of-the-art on downstream tasks.
      </p>
    </div>
  </div>
</section>
                

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Polysomnography (PSG) provides the gold standard for sleep assessment 
            but suffers from substantial heterogeneity across recording devices and cohorts. 
            There have been growing efforts to build general-purpose foundation models (FMs) for sleep physiology, 
            but lack an in-depth understanding of the pre-training process and scaling patterns that lead to more generalizable sleep FMs. 
          </p>
          <p>
            To fill this gap, we curate a massive corpus of 166,500 hours of sleep recordings 
            from nine public sources and establish SleepBench, a comprehensive, fully open-source benchmark. 
            Leveraging SleepBench, we systematically evaluate four families of self-supervised pre-training objectives
            and uncover three critical findings: (1) existing FMs fail to generalize to missing channels at inference; 
            (2) channel-invariant feature learning is essential for pre-training; 
            and (3) scaling sample size, model capacity, and multi-source data mixture consistently improves downstream performance. 
          </p>
          <p>
            With an enhanced pre-training and scaling recipe, we introduce OSF, 
            a family of sleep FMs that achieves state-of-the-art performance across nine datasets
            on diverse sleep and disease prediction tasks. 
            Further analysis of OSF also reveals intriguing properties in sample efficiency, hierarchical aggregation, 
            and cross-dataset scaling.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2>Motivation</h2>
      <hr>
      <h4>
        Which pre-training and scaling design choices truly improve the generalization of sleep FMs, 
        especially under cohort shift and missing-channel inference?
      </h4>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2>Main Findings</h2>
      <hr>

      <h3>Inference-time on Missing-channel is Challenging</h3>
      <div class="hero-body teaser-body">
      <figure class="image teaser-media">
        <img src="./static/images/3_motiv.svg" alt="Radar" />
      </figure>
      <p class="subtitle has-text-centered">
        Figure 2: Inference with full versus missing channels. Existing sleep FM fails on inference time missing channel samples.
      </p>
    </div>
      <p>
        Finding 1: Existing sleep FMs fail to generalize under missing-channel inference, 
        motivating pre-training designs that explicitly handle channel incompleteness.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="content">

      <h3>Channel-Invariant Pre-training Improves Robustness and Transfer</h3>
      <div class="hero-body teaser-body">
      <figure class="image teaser-media">
        <img src="./static/images/4_augmentation.svg" alt="Radar" />
      </figure>
      <p class="subtitle has-text-centered">
        Figure 3: Illustration of considered augmentations. We consider time-wise masking and channel masking strategies.
      </p>
    </div>
      <p>
        Finding 2: Explicitly encouraging channel-invariant feature learning during pre-training 
        improves robustness and downstream transfer, particularly for contrastive and distillation-based methods.
      </p>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h3>Scaling Behavior Holds in Sleep Data</h3>
      <div class="hero-body teaser-body">
      <figure class="image teaser-media">
        <img src="./static/images/model_scaling.svg" alt="Radar" />
      </figure>
      <p class="subtitle has-text-centered">
        Figure 4: Scaling behavior. Linear probing results on hypopnea detection show that 
        our OSF improves with both model capacity and pre-training sample size.
      </p>
    </div>
      <p>
        Finding 3: Scaling laws emerge in sleep data; jointly scaling model and data size yields the strongest gains.
      </p>
    </div>
  </div>
</section>


    
<section class="section">
  <div class="container is-max-desktop">
    <div class="content">
      <h2>Experiment Results</h2>
      <hr>
      <h3>Main Results</h3>
      <div class="hero-body teaser-body">
      <figure class="image table-media">
        <img src="./static/images/main_table.svg"/>
      </figure>
      <p class="subtitle has-text-centered">
        Table 1: Sleep staging and sleep event detection. OSF achieves the best overall performance among all compared methods.
      </p>
    </div>
      <p>
Guided by We evaluate models on four sleep event detection tasks. As shown in the table,
OSF achieves state-of-the-art performance on both sleep staging and event detection under linear probing and fine-tuning.
      </p>
    </div>
  </div>
</section>

    
<section class="section">
  <div class="container is-max-desktop">
    <div class="content">

      <h3>Missing-channel Inference</h3>
      <div class="hero-body teaser-body">
      <figure class="image table-media">
        <img src="./static/images/missing_inference.svg"/>
      </figure>
      <p class="subtitle has-text-centered">
        Table 2: Linear probing results under realistic missing-channel settings. OSF is more robust to missing channels.
      </p>
    </div>
      <p>
        As shown in table, OSF consistently outperforms SleepFM across these missing-channel settings. 
        Specifically, (1) OSF makes better use of the available channels.
        With brain-activity channels only, it achieves stronger sleep staging and arousal detection, 
        suggesting stronger brain-related representations. 
        Similarly, with respiratory channels only, it achieves stronger performance on hypopnea and oxygen desaturation. 
        (2) OSF is more robust when key modalities are missing. When respiratory signals are removed, 
        both methods degrade on hypopnea and oxygen desaturation, but OSF remains consistently better. 
        Conversely, when brain-related channels are unavailable, sleep staging becomes much harder for both models; 
        nevertheless, OSF better uses the remaining channels and yields stronger performance.
      </p>
    </div>
  </div>
</section>

    
    
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{shuai2026osf,
  title={OSF: On Pre-training and Scaling of Sleep Foudnation Model},
  author={Shuai, Zitao and Xu, Zongzhe and Yang, David and Wang, Wei and Yang, Yuzhe},
  journal={arXiv preprint},
  year={2026}
}</code></pre>
  </div>
</section>



</body>
</html>
